## [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)

A recurrent sequence-to-sequence feature prediction network and a modified WaveNet model for speech synthesis, achieving a mean opinion score (MOS) of 4.53. \[[blog](https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html)\]\[[samples](https://google.github.io/tacotron/)\]

### Notes
1. Instead of the Griffin-Lim algorithm as the vocoder as in the Tacotron system, Tacotron 2 uses a modified WaveNet as the vocoder.
2. Model architecture:
   - Two components:
     1. A recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence.
     2. A modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames.
   - Mel-frequency spectrograms are used, which allows the two components to be trained separately and is easier to train due to its invariance to phase within each frame.
   - Spectrogram prediction network:
     - This component consists of an encoder, which converts a character sequence into a hidden feature representation, and a decoder with attention, which predicts the spectrogram.
     - The exact structure of the encoder is similar to that in Tacotron: character embeddings and convolutional layers.
     - The location-sensitive attention is used, which encourages the model to move forward consistently through the input. This avoids potential failure modes where some subsequences are repeated or ignored by the decoder.
     - The decoder is an autoregressive recurrent neural network, which predicts the output spectrogram one frame at a time. A pre-net is used as an information bottleneck for the prediction from the previous time step. A stack of two uni-directional LSTM layers is used, and its output is concatenated with the attention context vector and projected through a linear transform to predict the target spectrogram frame and another linear transform to predict a "stop" token. A convolutional post-net is used to predict a residual that is added to the prediction to improve the overall reconstruction.
     - Much simpler building blocks compared to Tacotron, which used "CBHG" stacks and GRU recurrent layers.
   - WaveNet Vocoder:
     - This component is composed of dilated convolution layers to invert the mel spectrogram feature representation into time-domain waveform samples.
     - Instead of predicting discretized buckets with a softmax layer, a 10-component mixture of logistic distributions (MoL) is used to generate 16-bit samples at 24kHz.
3. Both components are trained separately. The WaveNet component is trained on the outputs generated by the first network. Teacher-forcing is used for the training of the first network.
4. For evaluation, human evaluation is conducted, similar to the original Tacotron. The proposed system is compared against several baselines, such as a WaveNet using linguistic features and the original Tacotron system. Tacotron 2 significantly outperforms the competing systems, with performance very similar to the ground truth. The results of a comparison experiment between Tacotron 2 and the ground truth show that a large proportion of people think that the generated samples and the ground truth are about the same.
5. The authors admit that there is still room for improvement in prosody modeling.
6. Another experiment on news headlines is conducted to test the generalization ability of Tacotron 2 to out-of-domain text. The results point to a challenge for end-to-end approaches, which is that they require training on data that cover intended usage.
7. Ablation studies are conducted:
   - A comparison is done between using the predicted features and the ground truth during the training of the WaveNet component. The results show that training using the predicted features obtains better performance. This is because the predicted spectrograms tend to be oversmoothed and less detailed compared to the ground truth, so training on these samples will make it easier for the system to generate waveforms during inference.
   - A comparison is done between using linear-frequency spectrograms and mel spectrograms. The performance is similar, but mel spectrograms are more compact so it is favored. It is also shown that a WaveNet vocoder produces much higher quality audio compared to Griffin-Lim.
   - A comparison is done between using and not using the post-net. The result shows that adding the post-net result in better performance.
   - A final study shows that the model is able to generate high-quality audio using very few layers with limited receptive field, showing that large receptive field size is not an essential factor for audio quality. However, the dilated convolutions are still needed to ensure sufficient context and generation quality.

### Thoughts
1. An extension to the original Tacotron. Differences/improvements:
   - Instead of the Griffin-Lim algorithm, a modified WaveNet model is used as the vocoder.
   - The original Tacotron uses building blocks such as "CBHG" stacks and GRU recurrent layers. Tacotron 2 uses vanilla LSTM and convolutional layers, which are much simpler.
   - A significant improvement to generated audio quality due to the WaveNet component.
2. As expected, a neural vocoder results in better generated quality.
3. Incredible performance, and is almost to the level of natural human speech. Example generated samples are available in the link at the top, labeled samples. Note that the authors stated that the reported MOS could be inflated since the evaluation set contain recurring patterns and common words in the training set.
4. It will be cool to be able to generate audio samples conditioned on attributes, like gender and/or accent. This should be possible, as shown in the original WaveNet paper.
5. A giant step towards natural conversation with AI agents!
