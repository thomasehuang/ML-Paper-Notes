## [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)

A GAN-based model with a cycle consistency loss for image-to-image translation with unpaired training data. \[[code](https://github.com/junyanz/CycleGAN)\]\[[website](https://junyanz.github.io/CycleGAN/)\]

### Notes
1. This paper proposes a method for image-to-image translation in the absence of paired training examples.
   - Motivation: paired training datasets can be difficult and expensive to obtain.
   - Adversarial loss is used to train the generator to produce images in the target domain. A cycle consistency loss is used to ensure the input and generated output are meaningfully paired.
2. Method:
   - The goal is to learning a mapping function between two domains. To accomplish this, the authors learn two mapping functions, one the inverse of the other. Addtionally, two adversarial discriminators are used, one for each mapping.
   - Two losses are used, adversarial loss and cycle consistency loss.
     - The classic adversarial loss is used to ensure the generated images look similar to the images in the target domain.
     - To ensure cycle-consistency, a corresponding loss is introduced. This loss is essentially the difference between the reconstructed image after translating the input and inverting back and the original input image. It combines both forward cycle consistency and backward cycle consistency.
3. For evaluation, the proposed method is compared against several baselines on the Cityscapes dataset, which consists of labels and photos, and Google Maps data, which consists of maps and aerial photos.
   - Metrics: AMT perceptual studies (human study), FCN score, semantic segmentation metrics (per-pixel accuracy, per-class accuracy, and mean class Intersection-Over-Union).
   - Baselines include methods for unpaired image-to-image translation (CoGAN, BiGAN, SimGAN, and feature loss + GAN) and a method trained on paired images (pix2pix).
   - For the Cityscapes dataset, the proposed method outperforms baselines for unpaired translation qualitatively. The generated samples are of similar quality to those generated by pix2pix, which is fully supervised by paired data. Quantitatively, the proposed method also outperforms baselines for unpaired translation in all the metrics listed above, and is able to fool AMT participants on around a quarter of the trials, while the baselines almost never fooled the participants.
   - The method is shown to generate good samples on a diverse range of datasets.
4. As an ablation study, the authors experiment with removing either the adversarial loss or the cycle consistency loss. Both resulted in a severe degradation in performance. The authors also experimented with having the cycle loss in only one direction, which resulted in worse performance due to training instability and mode collapse.
5. The proposed method is applied to several applications: collection style transfer, object transfiguration, season transfer, photo generation from paintings, and photo enhancement.
   - For some applications, an identity mapping loss is introduced to preserve the color of the input images.
6. Some limitations include geometric changes in images and unclosable gaps bewteen paired and unpaired methods.

### Thoughts
1. The results are really amazing. Since only two datasets are needed rather than a paired dataset, one can potentially learn a mapping between any two dataset (of course, the performance will vary). Someone even learned a mapping between human faces and ramen, with moderate success.
2. The simplicity of this approach is really attractive as well. The only addition is the cycle consistency loss, but that enables meaningful generations.
3. Can the same idea be applied to other translation tasks, like language translation or speech-to-text?
